Title: Predictive Model for Human Displacement as a Result of Natural Disasters

Overview:

In order to complete this project, I will need to complete three phases of work.  The first is gathering the data necessary to generate an accurate and precise model.  In order to do this, I will need to identify the large scale events necessary to create the model (1).  These events will include the two world wars, the Iranian revolution (2), the Taliban assuming control in Afghanistan (3, 4), the recent large scale forced population transfers in Syria, and the east to west migration after the tearing down of the Berlin Wall.  Where possible, I will filter the data by race/ethnicity in order to more accurately represent the movement.
	The second phase of the project is to build the predictive model.  I will use a supervised learning algorithm in order to complete this.  However, to use this algorithm, I will have to organize my data into clear inputs and outputs.  The inputs will be the relative “severity” of the event.  In order to determine severity objectively,  I will base the severity on the number of displaced persons as a result of the event.  For example, if 3 million people were displaced during event A, and 1 million people were displaced during event B, then event A would have a higher severity score than event B.  The output for the model would be the population density changes across the globe as a result of the event.  In order to do this, I will be using opencv to model the changes in population density.
	The final phase of the project is to train the predictive model based on the aforementioned inputs and outputs so that, once trained, will be able to take an event “severity” as an input along with a location, and from that be able to determine migration patterns as a result of the event.  This phase is able to be refined over time massively.  For example, the user may not know the severity of the event they are inputting, but rather know the scale of the event.  On that note, take a nuclear strike, a great fear nowadays, for example.  The average user would not know the relative severity of this matter, but rather knows that it is localized to a single city.  Because of this, my product will have to be refined in order to characterize typical events that users would want to see, and based on historical evidence predetermine their relative severities.  My focus with the refining of this project is to have this goal in mind: provide as much data as possible with as little user error as possible.  Having the user basically guess the severity of an event has a large margin of error, and therefore will undoubtedly have to be eliminated.
  
  
